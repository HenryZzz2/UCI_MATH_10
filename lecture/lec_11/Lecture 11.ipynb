{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 11: Logistic Regression and Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In classification problem, the response variables $y$ are discrete, representing different catagories. For simplicity, we will first introduce the **binary classification case** -- $y$ has only two categories, denoted as $0$ and $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assumption 1**: Dependent on the variable $x$, the response variable $y$ has different **probabilities** to take value in 0 or 1. Instead of predicting exact value of 0 or 1, we are actually predicting the **probabilities**.\n",
    "\n",
    "**Assumption 2**: Logistic function assumption. Given $x$, what is the probability to observe $y=1$?\n",
    "\n",
    "$$P(y=1|\\mathbf{x})=f(\\mathbf{x};\\mathbf{\\beta}) = \\frac{1}{1 + \\exp(-\\tilde{x}\\mathbf{\\beta})}\n",
    "=: \\sigma(\\tilde{x}\\mathbf{\\beta}). $$\n",
    "\n",
    "where $\\sigma(z)=\\frac{1}{1+\\exp{(-x)}}$ is called [standard logistic function](https://en.wikipedia.org/wiki/Logistic_regression#:~:text=Logistic%20regression%20is%20a%20statistical,a%20form%20of%20binary%20regression), or sigmoid function in deep learning. Recall that $\\beta\\in\\mathbb{R}^{p+1}$ and $\\tilde{x}$ is the \"augmented\" sample with first element one to incorporate intercept in the linear function.\n",
    "\n",
    "**Equivalent expression**:\n",
    "  - Denote $p = P(y=1|\\mathbf{x})$, then we can write in linear form\n",
    "  $$\\ln\\frac{p}{1-p}=\\tilde{x}\\beta$$\n",
    "  - Since $y$ only takes value in 0 or 1, we have\n",
    "  $$P(y|\\mathbf{x},\\beta) = f(\\mathbf{x};\\beta)^y \\big(1 - f(\\mathbf{x};\\beta) \\big)^{1-y}$$\n",
    "  \n",
    "**MLE (Maximum Likelihood Estimation)**\n",
    "\n",
    "Assume the samples are independent. The overall probibility to witness the whole training dataset\n",
    "$$\n",
    "{\\begin{aligned}\n",
    "&P(\\mathbf{y}\\; | \\; \\mathbf{X};\\beta )\\\\\n",
    "=&\\prod _{i=1}^N P\\left(y^{(i)}\\mid \\mathbf{x}^{(i)};\\beta\\right)\\\\\n",
    "=&\\prod_{i=1}^N f\\big(\\mathbf{x}^{(i)};\\beta \\big)^{y^{(i)}} \n",
    "\\Big(1-f\\big(\\mathbf{x}^{(i)};\\beta\\big) \\Big)^{\\big(1-y^{(i)}\\big)}\n",
    "\\end{aligned}}.\n",
    "$$\n",
    "\n",
    "By maximizing the logarithm of likelihood function, then we derive the **loss function** to be minimized\n",
    "$$\n",
    "L (\\beta) = L (\\beta; X,\\mathbf{y}) = - \\frac{1}{N}\\sum_{i=1}^N \n",
    "\\Bigl\\{y^{(i)} \\ln\\big( f(\\mathbf{x}^{(i)};\\beta) \\big) \n",
    "+ (1 - y^{(i)}) \\ln\\big( 1 - f(\\mathbf{x}^{(i)};\\beta) \\big) \\Bigr\\}.\n",
    "$$\n",
    "\n",
    "The loss function also has clear probabilistic interpretations. Given $i$-th sample, the vector of true labels $(y^{i},1-y^{i})$ can also be viewed as the probability distribution. Then the loss function is the mean of all [cross entropy](https://en.wikipedia.org/wiki/Cross_entropy) across samples, i.e. \"distance\" between true probability distribution and estimated probability distribution via logistic model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the gradient (left as exercise -- if you like)\n",
    "$$\n",
    "\\frac{\\partial L (\\beta)}{\\partial \\beta_{k}} =\\frac{1}{N}\\sum_{i=1}^N  \\big(\\sigma(\\tilde{x}^{(i)}\\beta)  - y^{(i)} \\big) \\tilde{x}^{(i)}_k.\n",
    "$$\n",
    "\n",
    "In vector form\n",
    "$$\n",
    "\\nabla_{\\beta} \\big( L (\\beta) \\big) = \\sum_{i=1}^N  \\big(\\sigma(\\tilde{x}^{(i)})  - y^{(i)} \\big) \\tilde{x}^{(i)} \n",
    "=\\frac{1}{N}\\sum_{i=1}^N \\big( f(\\mathbf{x}^{(i)};\\beta)  - y^{(i)} \\big) \\tilde{x}^{(i)}.\n",
    "$$\n",
    "\n",
    "This is still the nonlinear function of $\\beta$, indicating that we cannot derive something like \"normal equations\" in OLS. The solution here is [numerical optimization](https://github.com/Jaewan-Yun/optimizer-visualization).\n",
    "\n",
    "The simplest algorithm in optimization is [gradient descent (GD)](https://en.wikipedia.org/wiki/Gradient_descent#:~:text=Gradient%20descent%20is%20a%20first,function%20at%20the%20current%20point.). $$\\beta_{k+1}=\\beta_{k}-\\eta\\nabla L(\\beta).$$\n",
    "\n",
    "Here the step size $\\eta$ is also called **learning rate** in machine learning. Note that it is indeed the Euler's scheme to solve the ODE $$\\dot{\\beta} = -\\nabla L(\\beta).$$\n",
    "\n",
    "By setting certain stopping criterion for GD, we think that we have approximated the optimized solution $\\hat{\\beta}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions\n",
    "\n",
    "Now with the estimated $\\hat{\\beta}$ and given a new data $x^{new}$, we calculate the probability that $y^{new}=1$ as $f(\\mathbf{x};\\mathbf{\\beta})$. If is greater than 0.5, we assign that $y^{new}=1$. \n",
    "\n",
    "For the whole dataset, the **accuracy** is defined as ratio of number of correct predictions to the total number of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class myLogisticRegression():\n",
    "    \"\"\" Logistic Regression classifier -- this only works for the binary case.\n",
    "    Parameters:\n",
    "    -----------\n",
    "    learning_rate: float\n",
    "        The step length that will be taken when following the negative gradient during\n",
    "        training.\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate=.1):\n",
    "        \n",
    "        # learning rate can also be in the fit method\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "\n",
    "    def fit(self, data, y, n_iterations = 1000):\n",
    "        \"\"\" \n",
    "        don't forget the document string in methods\n",
    "        \"\"\"\n",
    "        ones = np.ones((data.shape[0],1)) # column of ones \n",
    "        X = np.concatenate((ones, data), axis = 1) # the augmented matrix, \\tilde{X} in our lecture\n",
    "        eta = self.learning_rate\n",
    "        \n",
    "        beta  = np.zeros(np.shape(X)[1]) # initialize beta, can be other choices\n",
    "\n",
    "        for k in range(n_iterations):\n",
    "            dbeta = self.loss_gradient(beta,X,y) # write another function to compute gradient\n",
    "            beta = beta - eta * dbeta # the formula of GD\n",
    "            # this step is optional -- just for inspection purposes\n",
    "            if k % 50 == 0: # plot weights and print loss every 50 steps\n",
    "                print(\"loss after\", k+1, \"iterations is: \", self.loss(beta,X,y))\n",
    "        \n",
    "        self.coeff = beta\n",
    "        \n",
    "    def predict(self, data):\n",
    "        ones = np.ones((data.shape[0],1)) # column of ones \n",
    "        X = np.concatenate((ones, data), axis = 1) # the augmented matrix, \\tilde{X} in our lecture\n",
    "        beta = self.coeff # the estimated beta\n",
    "        y_pred = np.round(self.sigmoid(np.dot(X,beta))).astype(int) # >0.5: ->1 else,->0 -- note that we always use Numpy universal functions when possible\n",
    "        return y_pred\n",
    "    \n",
    "    def score(self, data, y_true):\n",
    "        ones = np.ones((data.shape[0],1)) # column of ones \n",
    "        X = np.concatenate((ones, data), axis = 1) # the augmented matrix, \\tilde{X} in our lecture\n",
    "        y_pred = self.predict(data)\n",
    "        acc = np.mean(y_pred == y_true) # number of correct predictions/N\n",
    "        return acc\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "    \n",
    "    def loss(self,beta,X,y):\n",
    "        f_value = self.sigmoid(np.dot(X,beta))\n",
    "        loss_value = np.log(f_value + 1e-10) * y + (1.0 - y)* np.log(1 - f_value + 1e-10) # avoid nan issues\n",
    "        return -np.mean(loss_value)\n",
    "                          \n",
    "    def loss_gradient(self,beta,X,y):\n",
    "        f_value = self.sigmoid(np.dot(X,beta))                  \n",
    "        gradient_value = (f_value - y).reshape(-1,1)*X # this is the hardest expression -- check yourself\n",
    "        return np.mean(gradient_value, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "X, y = load_breast_cancer(return_X_y = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 30)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss after 1 iterations is:  0.7704000919325609\n",
      "loss after 51 iterations is:  0.9234485854021112\n",
      "loss after 101 iterations is:  0.540964240285867\n",
      "loss after 151 iterations is:  0.39124404681558\n",
      "loss after 201 iterations is:  0.37036060679508653\n",
      "loss after 251 iterations is:  0.3539655742179028\n",
      "loss after 301 iterations is:  0.34040008047058745\n",
      "loss after 351 iterations is:  0.3290043880434598\n",
      "loss after 401 iterations is:  0.3193284051902458\n",
      "loss after 451 iterations is:  0.311041748365799\n",
      "loss after 501 iterations is:  0.3038878556607375\n",
      "loss after 551 iterations is:  0.2976620578084048\n",
      "loss after 601 iterations is:  0.29220035526617916\n",
      "loss after 651 iterations is:  0.28737177960898685\n",
      "loss after 701 iterations is:  0.283071941752563\n",
      "loss after 751 iterations is:  0.2792174560306317\n",
      "loss after 801 iterations is:  0.2757413356503129\n",
      "loss after 851 iterations is:  0.2725893486410744\n",
      "loss after 901 iterations is:  0.2697172067962873\n",
      "loss after 951 iterations is:  0.26708842141622324\n",
      "loss after 1001 iterations is:  0.2646726705164665\n",
      "loss after 1051 iterations is:  0.2624445506630796\n",
      "loss after 1101 iterations is:  0.26038261621231595\n",
      "loss after 1151 iterations is:  0.25846863358441385\n",
      "loss after 1201 iterations is:  0.25668699731287803\n",
      "loss after 1251 iterations is:  0.25502426873929296\n",
      "loss after 1301 iterations is:  0.25346880849474185\n",
      "loss after 1351 iterations is:  0.252010481340019\n",
      "loss after 1401 iterations is:  0.2506404173232247\n",
      "loss after 1451 iterations is:  0.24935081713932872\n",
      "loss after 1501 iterations is:  0.24813479245950681\n",
      "loss after 1551 iterations is:  0.24698623413357196\n",
      "loss after 1601 iterations is:  0.2458997027646781\n",
      "loss after 1651 iterations is:  0.2448703373586748\n",
      "loss after 1701 iterations is:  0.24389377866553064\n",
      "loss after 1751 iterations is:  0.24296610453192358\n",
      "loss after 1801 iterations is:  0.24208377512640433\n",
      "loss after 1851 iterations is:  0.24124358632077714\n",
      "loss after 1901 iterations is:  0.24044262984237913\n",
      "loss after 1951 iterations is:  0.23967825907315843\n",
      "loss after 2001 iterations is:  0.2389480595788275\n",
      "loss after 2051 iterations is:  0.23824982361692804\n",
      "loss after 2101 iterations is:  0.23758152800553378\n",
      "loss after 2151 iterations is:  0.23694131484150163\n",
      "loss after 2201 iterations is:  0.23632747464405096\n",
      "loss after 2251 iterations is:  0.23573843157016208\n",
      "loss after 2301 iterations is:  0.23517273040608971\n",
      "loss after 2351 iterations is:  0.2346290250867296\n",
      "loss after 2401 iterations is:  0.23410606853366545\n",
      "loss after 2451 iterations is:  0.2336027036350486\n",
      "loss after 2501 iterations is:  0.23311785521728876\n",
      "loss after 2551 iterations is:  0.23265052288087484\n",
      "loss after 2601 iterations is:  0.23219977459131141\n",
      "loss after 2651 iterations is:  0.23176474093180433\n",
      "loss after 2701 iterations is:  0.23134460993748668\n",
      "loss after 2751 iterations is:  0.23093862244207525\n",
      "loss after 2801 iterations is:  0.23054606787724075\n",
      "loss after 2851 iterations is:  0.23016628047294055\n",
      "loss after 2901 iterations is:  0.2297986358137466\n",
      "loss after 2951 iterations is:  0.22944254771198905\n",
      "loss after 3001 iterations is:  0.22909746536348718\n",
      "loss after 3051 iterations is:  0.2287628707558928\n",
      "loss after 3101 iterations is:  0.2284382763033266\n",
      "loss after 3151 iterations is:  0.22812322268414426\n",
      "loss after 3201 iterations is:  0.22781727686139822\n",
      "loss after 3251 iterations is:  0.22752003026792791\n",
      "loss after 3301 iterations is:  0.22723109714006656\n",
      "loss after 3351 iterations is:  0.22695011298574785\n",
      "loss after 3401 iterations is:  0.22667673317435885\n",
      "loss after 3451 iterations is:  0.22641063163705674\n",
      "loss after 3501 iterations is:  0.22615149966747047\n",
      "loss after 3551 iterations is:  0.22589904481376322\n",
      "loss after 3601 iterations is:  0.2256529898539657\n",
      "loss after 3651 iterations is:  0.225413071847312\n",
      "loss after 3701 iterations is:  0.2251790412550359\n",
      "loss after 3751 iterations is:  0.22495066112473466\n",
      "loss after 3801 iterations is:  0.22472770633297662\n",
      "loss after 3851 iterations is:  0.2245099628813405\n",
      "loss after 3901 iterations is:  0.22429722724153084\n",
      "loss after 3951 iterations is:  0.22408930574561606\n",
      "loss after 4001 iterations is:  0.2238860140178029\n",
      "loss after 4051 iterations is:  0.2236871764444843\n",
      "loss after 4101 iterations is:  0.2234926256795902\n",
      "loss after 4151 iterations is:  0.22330220218253383\n",
      "loss after 4201 iterations is:  0.2231157537862827\n",
      "loss after 4251 iterations is:  0.22293313529329473\n",
      "loss after 4301 iterations is:  0.22275420809725582\n",
      "loss after 4351 iterations is:  0.22257883982872512\n",
      "loss after 4401 iterations is:  0.22240690402295463\n",
      "loss after 4451 iterations is:  0.22223827980829164\n",
      "loss after 4501 iterations is:  0.22207285161370108\n",
      "loss after 4551 iterations is:  0.221910508894064\n",
      "loss after 4601 iterations is:  0.2217511458720146\n",
      "loss after 4651 iterations is:  0.22159466129517544\n",
      "loss after 4701 iterations is:  0.22144095820774007\n",
      "loss after 4751 iterations is:  0.22128994373543348\n",
      "loss after 4801 iterations is:  0.22114152888295346\n",
      "loss after 4851 iterations is:  0.2209956283430659\n",
      "loss after 4901 iterations is:  0.2208521603165869\n",
      "loss after 4951 iterations is:  0.2207110463425429\n",
      "loss after 5001 iterations is:  0.22057221113785216\n",
      "loss after 5051 iterations is:  0.22043558244591702\n",
      "loss after 5101 iterations is:  0.22030109089356287\n",
      "loss after 5151 iterations is:  0.2201686698557982\n",
      "loss after 5201 iterations is:  0.22003825532790766\n",
      "loss after 5251 iterations is:  0.2199097858044261\n",
      "loss after 5301 iterations is:  0.2197832021645697\n",
      "loss after 5351 iterations is:  0.2196584475637328\n",
      "loss after 5401 iterations is:  0.21953546733068408\n",
      "loss after 5451 iterations is:  0.21941420887012092\n",
      "loss after 5501 iterations is:  0.2192946215702638\n",
      "loss after 5551 iterations is:  0.21917665671519426\n",
      "loss after 5601 iterations is:  0.21906026740165904\n",
      "loss after 5651 iterations is:  0.21894540846008023\n",
      "loss after 5701 iterations is:  0.21883203637953197\n",
      "loss after 5751 iterations is:  0.21872010923645435\n",
      "loss after 5801 iterations is:  0.21860958662689448\n",
      "loss after 5851 iterations is:  0.21850042960207539\n",
      "loss after 5901 iterations is:  0.21839260060710733\n",
      "loss after 5951 iterations is:  0.2182860634226673\n",
      "loss after 6001 iterations is:  0.21818078310948252\n",
      "loss after 6051 iterations is:  0.21807672595546598\n",
      "loss after 6101 iterations is:  0.21797385942535913\n",
      "loss after 6151 iterations is:  0.21787215211274702\n",
      "loss after 6201 iterations is:  0.21777157369431865\n",
      "loss after 6251 iterations is:  0.21767209488625383\n",
      "loss after 6301 iterations is:  0.21757368740262384\n",
      "loss after 6351 iterations is:  0.21747632391570018\n",
      "loss after 6401 iterations is:  0.21737997801807263\n",
      "loss after 6451 iterations is:  0.21728462418648273\n",
      "loss after 6501 iterations is:  0.21719023774728446\n",
      "loss after 6551 iterations is:  0.21709679484344968\n",
      "loss after 6601 iterations is:  0.21700427240303882\n",
      "loss after 6651 iterations is:  0.21691264810906466\n",
      "loss after 6701 iterations is:  0.21682190037067806\n",
      "loss after 6751 iterations is:  0.21673200829561037\n",
      "loss after 6801 iterations is:  0.21664295166381103\n",
      "loss after 6851 iterations is:  0.21655471090222078\n",
      "loss after 6901 iterations is:  0.21646726706062558\n",
      "loss after 6951 iterations is:  0.21638060178853874\n",
      "loss after 7001 iterations is:  0.21629469731306186\n",
      "loss after 7051 iterations is:  0.21620953641767704\n",
      "loss after 7101 iterations is:  0.21612510242192717\n",
      "loss after 7151 iterations is:  0.21604137916194133\n",
      "loss after 7201 iterations is:  0.21595835097176574\n",
      "loss after 7251 iterations is:  0.2158760026654628\n",
      "loss after 7301 iterations is:  0.21579431951994288\n",
      "loss after 7351 iterations is:  0.21571328725849367\n",
      "loss after 7401 iterations is:  0.21563289203497663\n",
      "loss after 7451 iterations is:  0.21555312041865932\n",
      "loss after 7501 iterations is:  0.21547395937965436\n",
      "loss after 7551 iterations is:  0.21539539627493845\n",
      "loss after 7601 iterations is:  0.21531741883492478\n",
      "loss after 7651 iterations is:  0.2152400151505644\n",
      "loss after 7701 iterations is:  0.21516317366095303\n",
      "loss after 7751 iterations is:  0.21508688314142094\n",
      "loss after 7801 iterations is:  0.21501113269208458\n",
      "loss after 7851 iterations is:  0.2149359117268402\n",
      "loss after 7901 iterations is:  0.21486120996277977\n",
      "loss after 7951 iterations is:  0.2147870174100109\n",
      "loss after 8001 iterations is:  0.21471332436186458\n",
      "loss after 8051 iterations is:  0.21464012138547167\n",
      "loss after 8101 iterations is:  0.2145673993126961\n",
      "loss after 8151 iterations is:  0.21449514923140528\n",
      "loss after 8201 iterations is:  0.21442336247706797\n",
      "loss after 8251 iterations is:  0.21435203062466218\n",
      "loss after 8301 iterations is:  0.214281145480883\n",
      "loss after 8351 iterations is:  0.2142106990766359\n",
      "loss after 8401 iterations is:  0.21414068365980485\n",
      "loss after 8451 iterations is:  0.21407109168828364\n",
      "loss after 8501 iterations is:  0.21400191582326006\n",
      "loss after 8551 iterations is:  0.2139331489227414\n",
      "loss after 8601 iterations is:  0.21386478403531362\n",
      "loss after 8651 iterations is:  0.21379681439412257\n",
      "loss after 8701 iterations is:  0.21372923341106992\n",
      "loss after 8751 iterations is:  0.21366203467121453\n",
      "loss after 8801 iterations is:  0.21359521192737155\n",
      "loss after 8851 iterations is:  0.21352875909490077\n",
      "loss after 8901 iterations is:  0.2134626702466775\n",
      "loss after 8951 iterations is:  0.2133969396082387\n",
      "loss after 9001 iterations is:  0.21333156155309688\n",
      "loss after 9051 iterations is:  0.21326653059821654\n",
      "loss after 9101 iterations is:  0.21320184139964504\n",
      "loss after 9151 iterations is:  0.2131374887482947\n",
      "loss after 9201 iterations is:  0.21307346756586648\n",
      "loss after 9251 iterations is:  0.2130097729009146\n",
      "loss after 9301 iterations is:  0.21294639992504216\n",
      "loss after 9351 iterations is:  0.21288334392922603\n",
      "loss after 9401 iterations is:  0.21282060032026467\n",
      "loss after 9451 iterations is:  0.21275816461734498\n",
      "loss after 9501 iterations is:  0.21269603244872282\n",
      "loss after 9551 iterations is:  0.21263419954851415\n",
      "loss after 9601 iterations is:  0.21257266175359205\n",
      "loss after 9651 iterations is:  0.21251141500058562\n",
      "loss after 9701 iterations is:  0.21245045532297724\n",
      "loss after 9751 iterations is:  0.21238977884829488\n",
      "loss after 9801 iterations is:  0.21232938179539576\n",
      "loss after 9851 iterations is:  0.2122692604718374\n",
      "loss after 9901 iterations is:  0.2122094112713345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss after 9951 iterations is:  0.21214983067129695\n",
      "loss after 10001 iterations is:  0.21209051523044697\n",
      "loss after 10051 iterations is:  0.21203146158651243\n",
      "loss after 10101 iterations is:  0.21197266645399337\n",
      "loss after 10151 iterations is:  0.2119141266219995\n",
      "loss after 10201 iterations is:  0.21185583895215604\n",
      "loss after 10251 iterations is:  0.21179780037657545\n",
      "loss after 10301 iterations is:  0.21174000789589287\n",
      "loss after 10351 iterations is:  0.2116824585773627\n",
      "loss after 10401 iterations is:  0.21162514955301487\n",
      "loss after 10451 iterations is:  0.21156807801786826\n",
      "loss after 10501 iterations is:  0.21151124122819925\n",
      "loss after 10551 iterations is:  0.21145463649986376\n",
      "loss after 10601 iterations is:  0.21139826120667116\n",
      "loss after 10651 iterations is:  0.2113421127788074\n",
      "loss after 10701 iterations is:  0.21128618870130694\n",
      "loss after 10751 iterations is:  0.2112304865125707\n",
      "loss after 10801 iterations is:  0.21117500380292942\n",
      "loss after 10851 iterations is:  0.21111973821325\n",
      "loss after 10901 iterations is:  0.21106468743358422\n",
      "loss after 10951 iterations is:  0.211009849201858\n",
      "loss after 11001 iterations is:  0.21095522130259972\n",
      "loss after 11051 iterations is:  0.21090080156570673\n",
      "loss after 11101 iterations is:  0.2108465878652485\n",
      "loss after 11151 iterations is:  0.2107925781183056\n",
      "loss after 11201 iterations is:  0.21073877028384222\n",
      "loss after 11251 iterations is:  0.21068516236161278\n",
      "loss after 11301 iterations is:  0.21063175239110038\n",
      "loss after 11351 iterations is:  0.21057853845048646\n",
      "loss after 11401 iterations is:  0.21052551865565028\n",
      "loss after 11451 iterations is:  0.2104726911591979\n",
      "loss after 11501 iterations is:  0.21042005414951925\n",
      "loss after 11551 iterations is:  0.21036760584987263\n",
      "loss after 11601 iterations is:  0.21031534451749534\n",
      "loss after 11651 iterations is:  0.2102632684427403\n",
      "loss after 11701 iterations is:  0.21021137594823727\n",
      "loss after 11751 iterations is:  0.21015966538807823\n",
      "loss after 11801 iterations is:  0.21010813514702575\n",
      "loss after 11851 iterations is:  0.21005678363974428\n",
      "loss after 11901 iterations is:  0.2100056093100529\n",
      "loss after 11951 iterations is:  0.20995461063019963\n",
      "loss after 12001 iterations is:  0.20990378610015575\n",
      "loss after 12051 iterations is:  0.20985313424693047\n",
      "loss after 12101 iterations is:  0.2098026536239046\n",
      "loss after 12151 iterations is:  0.20975234281018273\n",
      "loss after 12201 iterations is:  0.20970220040996387\n",
      "loss after 12251 iterations is:  0.20965222505192946\n",
      "loss after 12301 iterations is:  0.20960241538864838\n",
      "loss after 12351 iterations is:  0.2095527700959981\n",
      "loss after 12401 iterations is:  0.20950328787260258\n",
      "loss after 12451 iterations is:  0.20945396743928507\n",
      "loss after 12501 iterations is:  0.20940480753853602\n",
      "loss after 12551 iterations is:  0.20935580693399575\n",
      "loss after 12601 iterations is:  0.2093069644099512\n",
      "loss after 12651 iterations is:  0.2092582787708463\n",
      "loss after 12701 iterations is:  0.20920974884080606\n",
      "loss after 12751 iterations is:  0.20916137346317268\n",
      "loss after 12801 iterations is:  0.2091131515000553\n",
      "loss after 12851 iterations is:  0.2090650818318912\n",
      "loss after 12901 iterations is:  0.20901716335701895\n",
      "loss after 12951 iterations is:  0.2089693949912632\n",
      "loss after 13001 iterations is:  0.2089217756675304\n",
      "loss after 13051 iterations is:  0.20887430433541526\n",
      "loss after 13101 iterations is:  0.2088269799608182\n",
      "loss after 13151 iterations is:  0.20877980152557213\n",
      "loss after 13201 iterations is:  0.20873276802707993\n",
      "loss after 13251 iterations is:  0.20868587847796094\n",
      "loss after 13301 iterations is:  0.20863913190570738\n",
      "loss after 13351 iterations is:  0.20859252735234887\n",
      "loss after 13401 iterations is:  0.20854606387412689\n",
      "loss after 13451 iterations is:  0.2084997405411769\n",
      "loss after 13501 iterations is:  0.20845355643721925\n",
      "loss after 13551 iterations is:  0.20840751065925764\n",
      "loss after 13601 iterations is:  0.2083616023172863\n",
      "loss after 13651 iterations is:  0.2083158305340036\n",
      "loss after 13701 iterations is:  0.20827019444453418\n",
      "loss after 13751 iterations is:  0.2082246931961573\n",
      "loss after 13801 iterations is:  0.20817932594804317\n",
      "loss after 13851 iterations is:  0.2081340918709949\n",
      "loss after 13901 iterations is:  0.20808899014719823\n",
      "loss after 13951 iterations is:  0.20804401996997662\n",
      "loss after 14001 iterations is:  0.20799918054355343\n",
      "loss after 14051 iterations is:  0.20795447108281953\n",
      "loss after 14101 iterations is:  0.2079098908131073\n",
      "loss after 14151 iterations is:  0.20786543896996962\n",
      "loss after 14201 iterations is:  0.20782111479896564\n",
      "loss after 14251 iterations is:  0.2077769175554506\n",
      "loss after 14301 iterations is:  0.20773284650437182\n",
      "loss after 14351 iterations is:  0.20768890092006964\n",
      "loss after 14401 iterations is:  0.20764508008608284\n",
      "loss after 14451 iterations is:  0.2076013832949598\n",
      "loss after 14501 iterations is:  0.20755780984807357\n",
      "loss after 14551 iterations is:  0.20751435905544185\n",
      "loss after 14601 iterations is:  0.20747103023555144\n",
      "loss after 14651 iterations is:  0.20742782271518712\n",
      "loss after 14701 iterations is:  0.20738473582926412\n",
      "loss after 14751 iterations is:  0.20734176892066614\n",
      "loss after 14801 iterations is:  0.20729892134008523\n",
      "loss after 14851 iterations is:  0.20725619244586785\n",
      "loss after 14901 iterations is:  0.20721358160386266\n",
      "loss after 14951 iterations is:  0.2071710881872737\n",
      "loss after 15001 iterations is:  0.2071287115765159\n",
      "loss after 15051 iterations is:  0.20708645115907476\n",
      "loss after 15101 iterations is:  0.20704430632936927\n",
      "loss after 15151 iterations is:  0.2070022764886179\n",
      "loss after 15201 iterations is:  0.20696036104470833\n",
      "loss after 15251 iterations is:  0.20691855941206946\n",
      "loss after 15301 iterations is:  0.20687687101154756\n",
      "loss after 15351 iterations is:  0.20683529527028455\n",
      "loss after 15401 iterations is:  0.20679383162159926\n",
      "loss after 15451 iterations is:  0.20675247950487202\n",
      "loss after 15501 iterations is:  0.2067112383654315\n",
      "loss after 15551 iterations is:  0.20667010765444432\n",
      "loss after 15601 iterations is:  0.20662908682880735\n",
      "loss after 15651 iterations is:  0.20658817535104246\n",
      "loss after 15701 iterations is:  0.20654737268919388\n",
      "loss after 15751 iterations is:  0.20650667831672778\n",
      "loss after 15801 iterations is:  0.20646609171243424\n",
      "loss after 15851 iterations is:  0.20642561236033163\n",
      "loss after 15901 iterations is:  0.2063852397495728\n",
      "loss after 15951 iterations is:  0.20634497337435415\n",
      "loss after 16001 iterations is:  0.20630481273382617\n",
      "loss after 16051 iterations is:  0.20626475733200608\n",
      "loss after 16101 iterations is:  0.2062248066776931\n",
      "loss after 16151 iterations is:  0.20618496028438468\n",
      "loss after 16201 iterations is:  0.2061452176701955\n",
      "loss after 16251 iterations is:  0.20610557835777799\n",
      "loss after 16301 iterations is:  0.20606604187424438\n",
      "loss after 16351 iterations is:  0.20602660775109088\n",
      "loss after 16401 iterations is:  0.20598727552412371\n",
      "loss after 16451 iterations is:  0.20594804473338615\n",
      "loss after 16501 iterations is:  0.20590891492308783\n",
      "loss after 16551 iterations is:  0.2058698856415355\n",
      "loss after 16601 iterations is:  0.20583095644106494\n",
      "loss after 16651 iterations is:  0.20579212687797493\n",
      "loss after 16701 iterations is:  0.2057533965124623\n",
      "loss after 16751 iterations is:  0.20571476490855853\n",
      "loss after 16801 iterations is:  0.20567623163406812\n",
      "loss after 16851 iterations is:  0.20563779626050732\n",
      "loss after 16901 iterations is:  0.20559945836304536\n",
      "loss after 16951 iterations is:  0.20556121752044643\n",
      "loss after 17001 iterations is:  0.20552307331501238\n",
      "loss after 17051 iterations is:  0.20548502533252813\n",
      "loss after 17101 iterations is:  0.20544707316220645\n",
      "loss after 17151 iterations is:  0.2054092163966355\n",
      "loss after 17201 iterations is:  0.2053714546317266\n",
      "loss after 17251 iterations is:  0.20533378746666325\n",
      "loss after 17301 iterations is:  0.20529621450385133\n",
      "loss after 17351 iterations is:  0.2052587353488707\n",
      "loss after 17401 iterations is:  0.20522134961042726\n",
      "loss after 17451 iterations is:  0.20518405690030614\n",
      "loss after 17501 iterations is:  0.20514685683332629\n",
      "loss after 17551 iterations is:  0.2051097490272956\n",
      "loss after 17601 iterations is:  0.2050727331029671\n",
      "loss after 17651 iterations is:  0.205035808683996\n",
      "loss after 17701 iterations is:  0.20499897539689813\n",
      "loss after 17751 iterations is:  0.20496223287100807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss after 17801 iterations is:  0.20492558073843936\n",
      "loss after 17851 iterations is:  0.2048890186340453\n",
      "loss after 17901 iterations is:  0.20485254619537968\n",
      "loss after 17951 iterations is:  0.20481616306265932\n",
      "loss after 18001 iterations is:  0.20477986887872715\n",
      "loss after 18051 iterations is:  0.20474366328901544\n",
      "loss after 18101 iterations is:  0.20470754594151064\n",
      "loss after 18151 iterations is:  0.20467151648671822\n",
      "loss after 18201 iterations is:  0.20463557457762882\n",
      "loss after 18251 iterations is:  0.20459971986968434\n",
      "loss after 18301 iterations is:  0.20456395202074557\n",
      "loss after 18351 iterations is:  0.20452827069105972\n",
      "loss after 18401 iterations is:  0.20449267554322906\n",
      "loss after 18451 iterations is:  0.20445716624218\n",
      "loss after 18501 iterations is:  0.20442174245513262\n",
      "loss after 18551 iterations is:  0.20438640385157125\n",
      "loss after 18601 iterations is:  0.20435115010321517\n",
      "loss after 18651 iterations is:  0.2043159808839901\n",
      "loss after 18701 iterations is:  0.20428089587000015\n",
      "loss after 18751 iterations is:  0.20424589473950036\n",
      "loss after 18801 iterations is:  0.2042109771728699\n",
      "loss after 18851 iterations is:  0.2041761428525854\n",
      "loss after 18901 iterations is:  0.20414139146319532\n",
      "loss after 18951 iterations is:  0.20410672269129412\n",
      "loss after 19001 iterations is:  0.2040721362254978\n",
      "loss after 19051 iterations is:  0.20403763175641898\n",
      "loss after 19101 iterations is:  0.20400320897664329\n",
      "loss after 19151 iterations is:  0.2039688675807052\n",
      "loss after 19201 iterations is:  0.20393460726506565\n",
      "loss after 19251 iterations is:  0.2039004277280886\n",
      "loss after 19301 iterations is:  0.20386632867001941\n",
      "loss after 19351 iterations is:  0.2038323097929623\n",
      "loss after 19401 iterations is:  0.2037983708008592\n",
      "loss after 19451 iterations is:  0.20376451139946894\n",
      "loss after 19501 iterations is:  0.20373073129634595\n",
      "loss after 19551 iterations is:  0.2036970302008202\n",
      "loss after 19601 iterations is:  0.20366340782397732\n",
      "loss after 19651 iterations is:  0.20362986387863885\n",
      "loss after 19701 iterations is:  0.20359639807934296\n",
      "loss after 19751 iterations is:  0.20356301014232547\n",
      "loss after 19801 iterations is:  0.2035296997855015\n",
      "loss after 19851 iterations is:  0.20349646672844676\n",
      "loss after 19901 iterations is:  0.20346331069238022\n",
      "loss after 19951 iterations is:  0.20343023140014574\n"
     ]
    }
   ],
   "source": [
    "lg = myLogisticRegression(learning_rate=1e-5)\n",
    "lg.fit(X_train,y_train,n_iterations = 20000) # what about increase n_iterations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.916015625"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.66882874e-03,  1.83927907e-02, -4.36239390e-03,  8.66487934e-02,\n",
       "        1.49727981e-02,  5.50578751e-05, -6.71061493e-04, -1.14024608e-03,\n",
       "       -4.51040831e-04,  1.06678514e-04,  8.62208844e-05,  1.72299429e-04,\n",
       "        4.51211649e-04, -2.32558967e-03, -2.93966958e-02, -5.28628701e-06,\n",
       "       -1.83325756e-04, -2.46370803e-04, -5.80574855e-05, -9.52561495e-06,\n",
       "       -1.16457037e-05,  1.92488199e-02, -1.67105144e-02,  6.60427872e-02,\n",
       "       -2.88220491e-02, -2.09664782e-05, -2.48568195e-03, -3.30713576e-03,\n",
       "       -8.60537728e-04, -1.96407733e-04, -8.95680417e-05])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg.coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cliffzhou/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9824561403508771"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=0)\n",
    "clf.fit(X_train,y_train)\n",
    "clf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.953125"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's very normal that our result is different with sklearn. In sklearn [logistic regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression), they use different algorithms to solve the optimization problem , and even the model is different (adding regularization terms!)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
